#!/usr/bin/env python3
"""
Excel2Doc APIå­—æ®µä¸€è‡´æ€§æ£€æŸ¥è„šæœ¬

ç”¨é€”:
- æ£€æŸ¥å‰åç«¯APIå­—æ®µåä¸€è‡´æ€§
- è¯†åˆ«å‰ç«¯ç±»å‹å®šä¹‰ä¸åç«¯Schemaçš„ä¸åŒ¹é…
- éªŒè¯APIç«¯ç‚¹çš„è¯·æ±‚/å“åº”æ ¼å¼ä¸€è‡´æ€§
- ç”ŸæˆAPIä¸€è‡´æ€§æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
    python scripts/check_api_consistency.py
    python scripts/check_api_consistency.py --report  # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
"""

import os
import re
import sys
import json
import argparse
from pathlib import Path
from typing import List, Dict, Set, Optional, Any
from dataclasses import dataclass, field
from collections import defaultdict

@dataclass
class APIEndpoint:
    """APIç«¯ç‚¹ä¿¡æ¯"""
    method: str
    path: str
    file_path: str
    line_number: int
    request_fields: Set[str] = field(default_factory=set)
    response_fields: Set[str] = field(default_factory=set)

@dataclass
class TypeDefinition:
    """ç±»å‹å®šä¹‰ä¿¡æ¯"""
    name: str
    file_path: str
    line_number: int
    fields: Dict[str, str] = field(default_factory=dict)  # field_name -> field_type

@dataclass
class ConsistencyIssue:
    """ä¸€è‡´æ€§é—®é¢˜"""
    issue_type: str
    frontend_file: str
    backend_file: str
    frontend_line: int
    backend_line: int
    field_name: str
    frontend_format: str
    backend_format: str
    message: str
    severity: str  # 'error', 'warning', 'info'

class APIConsistencyChecker:
    """APIä¸€è‡´æ€§æ£€æŸ¥å™¨"""
    
    def __init__(self):
        self.frontend_types: Dict[str, TypeDefinition] = {}
        self.backend_schemas: Dict[str, TypeDefinition] = {}
        self.frontend_api_calls: List[Dict] = []
        self.backend_endpoints: List[APIEndpoint] = []
        self.issues: List[ConsistencyIssue] = []
        
        # å­—æ®µåæ˜ å°„è§„åˆ™
        self.field_mappings = {
            'created_at': 'createdAt',
            'updated_at': 'updatedAt',
            'excel_file': 'excelFile',
            'file_size': 'fileSize',
            'parse_status': 'parseStatus',
            'project_id': 'projectId',
            'column_name': 'columnName',
            'column_type': 'columnType',
            'is_active': 'isActive',
            'module_level': 'moduleLevel',
            'module_name': 'moduleName',
            'module_path': 'modulePath',
            'summary_content': 'content',
            'key_fields': 'keyFields',
            'extractor_type': 'extractorType',
            'generation_config': 'generationConfig'
        }

    def check_api_consistency(self) -> bool:
        """æ‰§è¡ŒAPIä¸€è‡´æ€§æ£€æŸ¥"""
        print("ğŸ” Excel2Doc APIä¸€è‡´æ€§æ£€æŸ¥å¼€å§‹...")
        print()
        
        # è§£æå‰ç«¯ç±»å‹å®šä¹‰
        print("ğŸ“‚ è§£æå‰ç«¯ç±»å‹å®šä¹‰...")
        self._parse_frontend_types()
        
        # è§£æåç«¯Schemaå®šä¹‰
        print("ğŸ“‚ è§£æåç«¯Schemaå®šä¹‰...")
        self._parse_backend_schemas()
        
        # è§£æå‰ç«¯APIè°ƒç”¨
        print("ğŸ“‚ è§£æå‰ç«¯APIè°ƒç”¨...")
        self._parse_frontend_api_calls()
        
        # è§£æåç«¯APIç«¯ç‚¹
        print("ğŸ“‚ è§£æåç«¯APIç«¯ç‚¹...")
        self._parse_backend_endpoints()
        
        # æ‰§è¡Œä¸€è‡´æ€§æ£€æŸ¥
        print("ğŸ” æ‰§è¡Œä¸€è‡´æ€§æ£€æŸ¥...")
        self._check_type_consistency()
        self._check_api_field_consistency()
        self._check_enum_consistency()
        
        # è¾“å‡ºæ£€æŸ¥ç»“æœ
        return self._report_results()

    def _parse_frontend_types(self) -> None:
        """è§£æå‰ç«¯TypeScriptç±»å‹å®šä¹‰"""
        frontend_types_dir = Path("frontend/src/types")
        if not frontend_types_dir.exists():
            return
            
        for file_path in frontend_types_dir.rglob("*.ts"):
            self._parse_typescript_file(file_path)

    def _parse_typescript_file(self, file_path: Path) -> None:
        """è§£æTypeScriptæ–‡ä»¶ä¸­çš„ç±»å‹å®šä¹‰"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except (UnicodeDecodeError, FileNotFoundError):
            return

        # åŒ¹é…æ¥å£å®šä¹‰
        interface_pattern = r'export\s+interface\s+(\w+)\s*\{([^}]*)\}'
        interfaces = re.finditer(interface_pattern, content, re.DOTALL)
        
        for match in interfaces:
            interface_name = match.group(1)
            interface_body = match.group(2)
            line_num = self._get_line_number(content, match.start())
            
            # è§£æå­—æ®µ
            field_pattern = r'(\w+)\s*\??\s*:\s*([^;,\n]+)'
            fields = {}
            for field_match in re.finditer(field_pattern, interface_body):
                field_name = field_match.group(1).strip()
                field_type = field_match.group(2).strip()
                fields[field_name] = field_type
            
            self.frontend_types[interface_name] = TypeDefinition(
                name=interface_name,
                file_path=str(file_path),
                line_number=line_num,
                fields=fields
            )

    def _parse_backend_schemas(self) -> None:
        """è§£æåç«¯Pydantic Schemaå®šä¹‰"""
        backend_schemas_dir = Path("backend/app/schemas")
        if not backend_schemas_dir.exists():
            return
            
        for file_path in backend_schemas_dir.rglob("*.py"):
            self._parse_pydantic_file(file_path)

    def _parse_pydantic_file(self, file_path: Path) -> None:
        """è§£æPydantic Schemaæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except (UnicodeDecodeError, FileNotFoundError):
            return

        # åŒ¹é…Pydanticæ¨¡å‹å®šä¹‰
        class_pattern = r'class\s+(\w+)\(BaseModel\):([^}]*?)(?=\n\nclass|\n\ndef|\Z)'
        classes = re.finditer(class_pattern, content, re.DOTALL)
        
        for match in classes:
            class_name = match.group(1)
            class_body = match.group(2)
            line_num = self._get_line_number(content, match.start())
            
            # è§£æå­—æ®µå’Œalias
            field_pattern = r'(\w+)\s*:\s*([^=\n]+)(?:\s*=\s*Field\([^)]*alias\s*=\s*[\'"](\w+)[\'"][^)]*\))?'
            fields = {}
            for field_match in re.finditer(field_pattern, class_body):
                field_name = field_match.group(1).strip()
                field_type = field_match.group(2).strip()
                alias = field_match.group(3) if len(field_match.groups()) > 2 and field_match.group(3) else None
                
                # è®°å½•å­—æ®µåï¼ˆä¼˜å…ˆä½¿ç”¨aliasï¼‰
                display_name = alias if alias else field_name
                fields[display_name] = field_type
                
                # åŒæ—¶è®°å½•åŸå­—æ®µåï¼Œç”¨äºæ£€æŸ¥ä¸€è‡´æ€§
                if alias:
                    fields[f"_original_{field_name}"] = field_type
            
            self.backend_schemas[class_name] = TypeDefinition(
                name=class_name,
                file_path=str(file_path),
                line_number=line_num,
                fields=fields
            )

    def _parse_frontend_api_calls(self) -> None:
        """è§£æå‰ç«¯APIè°ƒç”¨"""
        services_dir = Path("frontend/src/services")
        if not services_dir.exists():
            return
            
        for file_path in services_dir.rglob("*.ts"):
            self._parse_api_service_file(file_path)

    def _parse_api_service_file(self, file_path: Path) -> None:
        """è§£æAPIæœåŠ¡æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except (UnicodeDecodeError, FileNotFoundError):
            return

        # æŸ¥æ‰¾APIè°ƒç”¨æ¨¡å¼
        api_call_pattern = r'api\.(get|post|put|delete)\s*\(\s*[\'"]([^\'"]*)[\'"][^)]*\)'
        api_calls = re.finditer(api_call_pattern, content)
        
        for match in api_calls:
            method = match.group(1).upper()
            path = match.group(2)
            line_num = self._get_line_number(content, match.start())
            
            self.frontend_api_calls.append({
                'method': method,
                'path': path,
                'file': str(file_path),
                'line': line_num
            })

    def _parse_backend_endpoints(self) -> None:
        """è§£æåç«¯APIç«¯ç‚¹"""
        api_dir = Path("backend/app/api")
        if not api_dir.exists():
            return
            
        for file_path in api_dir.rglob("*.py"):
            self._parse_fastapi_file(file_path)

    def _parse_fastapi_file(self, file_path: Path) -> None:
        """è§£æFastAPIè·¯ç”±æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except (UnicodeDecodeError, FileNotFoundError):
            return

        # åŒ¹é…è·¯ç”±å®šä¹‰
        route_pattern = r'@router\.(get|post|put|delete)\s*\(\s*[\'"]([^\'"]*)[\'"]'
        routes = re.finditer(route_pattern, content)
        
        for match in routes:
            method = match.group(1).upper()
            path = match.group(2)
            line_num = self._get_line_number(content, match.start())
            
            endpoint = APIEndpoint(
                method=method,
                path=path,
                file_path=str(file_path),
                line_number=line_num
            )
            self.backend_endpoints.append(endpoint)

    def _check_type_consistency(self) -> None:
        """æ£€æŸ¥ç±»å‹å®šä¹‰ä¸€è‡´æ€§"""
        # æ¯”è¾ƒç›¸ä¼¼åç§°çš„å‰åç«¯ç±»å‹
        for frontend_name, frontend_type in self.frontend_types.items():
            # æŸ¥æ‰¾å¯¹åº”çš„åç«¯Schema
            backend_candidates = []
            for backend_name, backend_type in self.backend_schemas.items():
                if self._is_related_type(frontend_name, backend_name):
                    backend_candidates.append((backend_name, backend_type))
            
            for backend_name, backend_type in backend_candidates:
                self._compare_type_fields(frontend_type, backend_type)

    def _is_related_type(self, frontend_name: str, backend_name: str) -> bool:
        """åˆ¤æ–­å‰åç«¯ç±»å‹æ˜¯å¦ç›¸å…³"""
        # ç®€å•çš„åç§°åŒ¹é…é€»è¾‘
        frontend_lower = frontend_name.lower()
        backend_lower = backend_name.lower()
        
        # ç§»é™¤å¸¸è§åç¼€
        for suffix in ['read', 'create', 'update', 'response', 'request']:
            backend_lower = backend_lower.replace(suffix, '')
        
        return (frontend_lower in backend_lower or 
                backend_lower in frontend_lower or
                abs(len(frontend_lower) - len(backend_lower)) <= 3)

    def _compare_type_fields(self, frontend_type: TypeDefinition, backend_type: TypeDefinition) -> None:
        """æ¯”è¾ƒå‰åç«¯ç±»å‹å­—æ®µ"""
        frontend_fields = set(frontend_type.fields.keys())
        backend_fields = set(k for k in backend_type.fields.keys() if not k.startswith('_original_'))
        
        # æ£€æŸ¥å­—æ®µåä¸€è‡´æ€§
        for field_name in frontend_fields:
            if field_name in backend_fields:
                continue  # å­—æ®µåå®Œå…¨åŒ¹é…ï¼Œæ­£å¸¸æƒ…å†µ
                
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥æœ‰å¯¹åº”çš„snake_caseç‰ˆæœ¬
            snake_version = self._camel_to_snake(field_name)
            if f"_original_{snake_version}" in backend_type.fields:
                # åç«¯æœ‰å¯¹åº”çš„snake_caseå­—æ®µï¼Œä½†æ²¡æœ‰æ­£ç¡®çš„alias
                self.issues.append(ConsistencyIssue(
                    issue_type='missing_alias',
                    frontend_file=frontend_type.file_path,
                    backend_file=backend_type.file_path,
                    frontend_line=frontend_type.line_number,
                    backend_line=backend_type.line_number,
                    field_name=field_name,
                    frontend_format=field_name,
                    backend_format=snake_version,
                    message=f'Backend field "{snake_version}" should have camelCase alias "{field_name}"',
                    severity='warning'
                ))

        # æ£€æŸ¥åç«¯æœ‰è€Œå‰ç«¯æ²¡æœ‰çš„å­—æ®µ
        for field_name in backend_fields:
            if field_name not in frontend_fields:
                original_field = f"_original_{self._camel_to_snake(field_name)}"
                if original_field in backend_type.fields:
                    self.issues.append(ConsistencyIssue(
                        issue_type='missing_frontend_field',
                        frontend_file=frontend_type.file_path,
                        backend_file=backend_type.file_path,
                        frontend_line=frontend_type.line_number,
                        backend_line=backend_type.line_number,
                        field_name=field_name,
                        frontend_format='missing',
                        backend_format=field_name,
                        message=f'Frontend type "{frontend_type.name}" missing field "{field_name}"',
                        severity='info'
                    ))

    def _check_api_field_consistency(self) -> None:
        """æ£€æŸ¥APIå­—æ®µä¸€è‡´æ€§"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„APIå­—æ®µæ£€æŸ¥é€»è¾‘
        # ç”±äºè§£æAPIè¯·æ±‚/å“åº”ä½“æ¯”è¾ƒå¤æ‚ï¼Œæš‚æ—¶è·³è¿‡è¯¦ç»†å®ç°
        pass

    def _check_enum_consistency(self) -> None:
        """æ£€æŸ¥æšä¸¾å€¼ä¸€è‡´æ€§"""
        # 1. æ”¶é›†æ‰€æœ‰æšä¸¾å®šä¹‰
        backend_enums = self._collect_backend_enums()
        frontend_enums = self._collect_frontend_enums()
        
        # 2. æ£€æŸ¥ç›¸åŒæ¦‚å¿µçš„æšä¸¾æ˜¯å¦å€¼ä¸€è‡´
        for backend_enum_name, backend_values in backend_enums.items():
            # å¯»æ‰¾å‰ç«¯å¯¹åº”çš„æšä¸¾
            frontend_counterpart = self._find_frontend_enum_counterpart(
                backend_enum_name, frontend_enums
            )
            
            if frontend_counterpart:
                frontend_values = frontend_enums[frontend_counterpart]
                
                # æ¯”è¾ƒå€¼
                missing_in_frontend = set(backend_values.values()) - set(frontend_values.values())
                missing_in_backend = set(frontend_values.values()) - set(backend_values.values())
                
                if missing_in_frontend or missing_in_backend:
                    self.issues.append(ConsistencyIssue(
                        issue_type='enum_value_mismatch',
                        frontend_file='multiple frontend files',
                        backend_file='multiple backend files',
                        frontend_line=0,
                        backend_line=0,
                        field_name=f'{backend_enum_name}/{frontend_counterpart}',
                        frontend_format=f'values: {list(frontend_values.values())}',
                        backend_format=f'values: {list(backend_values.values())}',
                        message=f'æšä¸¾å€¼ä¸åŒ¹é…: å‰ç«¯ {frontend_counterpart} vs åç«¯ {backend_enum_name}',
                        severity='error'
                    ))
        
        # 3. æŸ¥æ‰¾å¯èƒ½åº”è¯¥ä½¿ç”¨æšä¸¾çš„ç¡¬ç¼–ç å­—ç¬¦ä¸²
        hardcoded_candidates = self._find_hardcoded_enum_candidates()
        for candidate in hardcoded_candidates:
            self.issues.append(ConsistencyIssue(
                issue_type='hardcoded_enum_candidate',
                frontend_file='',
                backend_file=candidate['file_path'],
                frontend_line=0,
                backend_line=candidate['line_number'],
                field_name=candidate['string_value'],
                frontend_format='',
                backend_format=f'ç¡¬ç¼–ç å­—ç¬¦ä¸²: "{candidate["string_value"]}"',
                message=f'å¯èƒ½åº”è¯¥ä½¿ç”¨æšä¸¾: "{candidate["string_value"]}" (ç±»å‹: {candidate["suggested_enum_type"]})',
                severity='warning'
            ))

    def _collect_backend_enums(self) -> Dict[str, Dict[str, str]]:
        """æ”¶é›†åç«¯æšä¸¾å®šä¹‰"""
        enums = {}
        backend_dir = Path("backend/app")
        
        if not backend_dir.exists():
            return enums
            
        for py_file in backend_dir.rglob("*.py"):
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                # æŸ¥æ‰¾æšä¸¾ç±»å®šä¹‰
                enum_pattern = r'class\s+(\w+Enum).*?:\s*(.*?)(?=\n\n|\nclass|\ndef|\nif|\Z)'
                enum_matches = re.finditer(enum_pattern, content, re.DOTALL)
                
                for match in enum_matches:
                    enum_name = match.group(1)
                    enum_body = match.group(2)
                    
                    # æå–æšä¸¾å€¼
                    enum_values = {}
                    value_pattern = r'(\w+)\s*=\s*[\'"]([^\'"]*)[\'"]'
                    value_matches = re.finditer(value_pattern, enum_body)
                    
                    for value_match in value_matches:
                        key = value_match.group(1)
                        value = value_match.group(2)
                        enum_values[key] = value
                    
                    if enum_values:
                        enums[enum_name] = enum_values
                        
            except (UnicodeDecodeError, FileNotFoundError):
                continue
                
        return enums

    def _collect_frontend_enums(self) -> Dict[str, Dict[str, str]]:
        """æ”¶é›†å‰ç«¯æšä¸¾å®šä¹‰"""
        enums = {}
        frontend_dir = Path("frontend/src")
        
        if not frontend_dir.exists():
            return enums
            
        for ts_file in frontend_dir.rglob("*.ts"):
            try:
                with open(ts_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                # æŸ¥æ‰¾enumå®šä¹‰
                enum_pattern = r'enum\s+(\w+)\s*\{([^}]*)\}'
                enum_matches = re.finditer(enum_pattern, content, re.DOTALL)
                
                for match in enum_matches:
                    enum_name = match.group(1)
                    enum_body = match.group(2)
                    
                    # æå–æšä¸¾å€¼
                    enum_values = {}
                    value_pattern = r'(\w+)\s*=\s*[\'"]([^\'"]*)[\'"]'
                    value_matches = re.finditer(value_pattern, enum_body)
                    
                    for value_match in value_matches:
                        key = value_match.group(1)
                        value = value_match.group(2)
                        enum_values[key] = value
                    
                    if enum_values:
                        enums[enum_name] = enum_values
                
                # æŸ¥æ‰¾typeè”åˆç±»å‹
                type_pattern = r'type\s+(\w+)\s*=\s*([^;]+);'
                type_matches = re.finditer(type_pattern, content)
                
                for match in type_matches:
                    type_name = match.group(1)
                    type_def = match.group(2).strip()
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å­—ç¬¦ä¸²è”åˆç±»å‹
                    if "'" in type_def or '"' in type_def:
                        string_values = re.findall(r'[\'"]([^\'"]*)[\'"]', type_def)
                        if string_values:
                            enum_values = {f"VALUE_{i}": value for i, value in enumerate(string_values)}
                            enums[type_name] = enum_values
                            
            except (UnicodeDecodeError, FileNotFoundError):
                continue
                
        return enums

    def _find_frontend_enum_counterpart(self, backend_enum_name: str, frontend_enums: Dict[str, Dict[str, str]]) -> Optional[str]:
        """å¯»æ‰¾å‰ç«¯å¯¹åº”çš„æšä¸¾"""
        backend_lower = backend_enum_name.lower().replace('enum', '')
        
        for frontend_name in frontend_enums.keys():
            frontend_lower = frontend_name.lower().replace('enum', '')
            
            # æ£€æŸ¥åç§°ç›¸ä¼¼æ€§
            if (backend_lower in frontend_lower or 
                frontend_lower in backend_lower or
                self._calculate_similarity(backend_lower, frontend_lower) > 0.7):
                return frontend_name
        
        return None

    def _calculate_similarity(self, str1: str, str2: str) -> float:
        """è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦"""
        from difflib import SequenceMatcher
        return SequenceMatcher(None, str1, str2).ratio()

    def _find_hardcoded_enum_candidates(self) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾å¯èƒ½åº”è¯¥ä½¿ç”¨æšä¸¾çš„ç¡¬ç¼–ç å­—ç¬¦ä¸²"""
        candidates = []
        
        # å®šä¹‰æšä¸¾å€¼æ¨¡å¼
        enum_patterns = {
            'parse_status': r'^(uploaded|parsing|completed|failed)$',
            'task_status': r'^(pending|running|completed|failed|cancelled)$', 
            'user_status': r'^(active|inactive|pending|suspended)$',
            'log_level': r'^(debug|info|warning|error|fatal)$',
            'file_type': r'^(excel|csv|json|xml|pdf)$'
        }
        
        # æ‰«æåç«¯æ–‡ä»¶
        backend_dir = Path("backend/app")
        if backend_dir.exists():
            for py_file in backend_dir.rglob("*.py"):
                try:
                    with open(py_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # æŸ¥æ‰¾å­—ç¬¦ä¸²å­—é¢é‡
                    string_pattern = r'[\'"]([^\'"]+)[\'"]'
                    matches = re.finditer(string_pattern, content)
                    
                    for match in matches:
                        string_value = match.group(1)
                        
                        # è·³è¿‡æ˜æ˜¾ä¸æ˜¯æšä¸¾çš„å­—ç¬¦ä¸²
                        if (len(string_value) < 2 or len(string_value) > 20 or
                            string_value.startswith(('http', '/', '.', '_')) or
                            string_value in {'utf-8', 'application/json', 'GET', 'POST'}):
                            continue
                        
                        # æ£€æŸ¥æ˜¯å¦åŒ¹é…æšä¸¾æ¨¡å¼
                        for enum_type, pattern in enum_patterns.items():
                            if re.match(pattern, string_value.lower()):
                                line_num = content[:match.start()].count('\n') + 1
                                context = self._get_line_context(content, match.start())
                                
                                candidates.append({
                                    'file_path': str(py_file),
                                    'line_number': line_num,
                                    'string_value': string_value,
                                    'suggested_enum_type': enum_type,
                                    'context': context
                                })
                                break
                                
                except (UnicodeDecodeError, FileNotFoundError):
                    continue
        
        return candidates

    def _get_line_context(self, content: str, position: int) -> str:
        """è·å–æŒ‡å®šä½ç½®çš„ä»£ç ä¸Šä¸‹æ–‡"""
        lines = content[:position].split('\n')
        line_num = len(lines) - 1
        all_lines = content.split('\n')
        
        if 0 <= line_num < len(all_lines):
            return all_lines[line_num].strip()
        
        return ""

    def _camel_to_snake(self, camel_str: str) -> str:
        """å°†camelCaseè½¬æ¢ä¸ºsnake_case"""
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', camel_str)
        return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()

    def _get_line_number(self, content: str, position: int) -> int:
        """æ ¹æ®å­—ç¬¦ä½ç½®è®¡ç®—è¡Œå·"""
        return content[:position].count('\n') + 1

    def _report_results(self) -> bool:
        """è¾“å‡ºæ£€æŸ¥ç»“æœ"""
        print()
        
        if not self.issues:
            print("âœ… å‰åç«¯APIå­—æ®µæ ¼å¼å®Œå…¨ä¸€è‡´!")
            print()
            self._print_summary()
            return True

        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
        error_issues = [i for i in self.issues if i.severity == 'error']
        warning_issues = [i for i in self.issues if i.severity == 'warning']
        info_issues = [i for i in self.issues if i.severity == 'info']
        
        print(f"âŒ å‘ç° {len(self.issues)} ä¸ªAPIä¸€è‡´æ€§é—®é¢˜:")
        print(f"   ğŸ”´ é”™è¯¯: {len(error_issues)} ä¸ª")
        print(f"   ğŸŸ¡ è­¦å‘Š: {len(warning_issues)} ä¸ª")  
        print(f"   â„¹ï¸  ä¿¡æ¯: {len(info_issues)} ä¸ª")
        print()

        # æŒ‰é—®é¢˜ç±»å‹åˆ†ç»„æ˜¾ç¤º
        issues_by_type = defaultdict(list)
        for issue in self.issues:
            issues_by_type[issue.issue_type].append(issue)

        for issue_type, type_issues in issues_by_type.items():
            print(f"ğŸ“‚ {issue_type.replace('_', ' ').title()} ({len(type_issues)} ä¸ª):")
            for issue in type_issues:
                severity_icon = {'error': 'ğŸ”´', 'warning': 'ğŸŸ¡', 'info': 'â„¹ï¸'}[issue.severity]
                print(f"   {severity_icon} {issue.message}")
                print(f"      å‰ç«¯: {issue.frontend_file}:{issue.frontend_line}")
                print(f"      åç«¯: {issue.backend_file}:{issue.backend_line}")
            print()

        self._print_summary()
        return len(error_issues) == 0

    def _print_summary(self) -> None:
        """è¾“å‡ºè§£æç»Ÿè®¡ä¿¡æ¯"""
        print("ğŸ“Š è§£æç»Ÿè®¡:")
        print(f"   å‰ç«¯ç±»å‹å®šä¹‰: {len(self.frontend_types)} ä¸ª")
        print(f"   åç«¯Schemaå®šä¹‰: {len(self.backend_schemas)} ä¸ª")
        print(f"   å‰ç«¯APIè°ƒç”¨: {len(self.frontend_api_calls)} ä¸ª")
        print(f"   åç«¯APIç«¯ç‚¹: {len(self.backend_endpoints)} ä¸ª")
        print()
        
        if self.frontend_types:
            print("ğŸ“‹ å‰ç«¯ç±»å‹:")
            for name in sorted(self.frontend_types.keys()):
                field_count = len(self.frontend_types[name].fields)
                print(f"   {name} ({field_count} å­—æ®µ)")
        
        if self.backend_schemas:
            print("ğŸ“‹ åç«¯Schema:")
            for name in sorted(self.backend_schemas.keys()):
                field_count = len([k for k in self.backend_schemas[name].fields.keys() 
                                 if not k.startswith('_original_')])
                print(f"   {name} ({field_count} å­—æ®µ)")
        print()

def main():
    parser = argparse.ArgumentParser(description='Excel2Doc APIä¸€è‡´æ€§æ£€æŸ¥å·¥å…·')
    parser.add_argument('--report', action='store_true', help='ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š')
    parser.add_argument('--verbose', '-v', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')
    
    args = parser.parse_args()
    
    if not os.path.exists('frontend') and not os.path.exists('backend'):
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°frontendæˆ–backendç›®å½•")
        print("è¯·åœ¨Excel2Docé¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬")
        sys.exit(1)

    checker = APIConsistencyChecker()
    success = checker.check_api_consistency()
    
    if args.report:
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = "api_consistency_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            report_data = {
                'frontend_types': {name: {
                    'file': type_def.file_path,
                    'fields': list(type_def.fields.keys())
                } for name, type_def in checker.frontend_types.items()},
                'backend_schemas': {name: {
                    'file': type_def.file_path,
                    'fields': [k for k in type_def.fields.keys() if not k.startswith('_original_')]
                } for name, type_def in checker.backend_schemas.items()},
                'issues': [{
                    'type': issue.issue_type,
                    'message': issue.message,
                    'severity': issue.severity,
                    'frontend_file': issue.frontend_file,
                    'backend_file': issue.backend_file
                } for issue in checker.issues]
            }
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()