#!/usr/bin/env python3
"""
Excel2Doc æšä¸¾ä½¿ç”¨åœºæ™¯é™æ€åˆ†æå™¨

ç”¨é€”:
- åˆ†ææ‰€æœ‰æšä¸¾å®šä¹‰åŠå…¶å€¼
- è¿½è¸ªæšä¸¾å€¼åœ¨ä»£ç ä¸­çš„ä½¿ç”¨æƒ…å†µ  
- æ£€æŸ¥æ˜¯å¦æœ‰ç¡¬ç¼–ç å­—ç¬¦ä¸²åº”è¯¥ä½¿ç”¨æšä¸¾
- éªŒè¯å‰åç«¯æšä¸¾ä½¿ç”¨ä¸€è‡´æ€§

ä½¿ç”¨æ–¹æ³•:
    python scripts/enum_usage_analyzer.py
    python scripts/enum_usage_analyzer.py --verbose      # è¯¦ç»†è¾“å‡º
    python scripts/enum_usage_analyzer.py --report       # ç”Ÿæˆåˆ†ææŠ¥å‘Š
"""

import ast
import re
import json
from pathlib import Path
from typing import Dict, List, Set, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict
from datetime import datetime
import argparse

@dataclass
class EnumDefinition:
    """æšä¸¾å®šä¹‰ä¿¡æ¯"""
    name: str
    file_path: str
    line_number: int
    values: Dict[str, str]  # key -> value
    base_classes: List[str]
    language: str  # 'python' or 'typescript'

@dataclass
class EnumUsage:
    """æšä¸¾ä½¿ç”¨ä¿¡æ¯"""
    enum_name: str
    usage_type: str  # 'direct', 'string_literal', 'hardcoded'
    file_path: str
    line_number: int
    context: str
    value: str
    language: str

@dataclass
class ConsistencyIssue:
    """ä¸€è‡´æ€§é—®é¢˜"""
    issue_type: str
    description: str
    frontend_item: Optional[str] = None
    backend_item: Optional[str] = None
    severity: str = 'warning'  # 'error', 'warning', 'info'

class EnumUsageAnalyzer:
    """æšä¸¾ä½¿ç”¨åœºæ™¯åˆ†æå™¨"""
    
    def __init__(self, verbose: bool = False, generate_report: bool = False):
        self.verbose = verbose
        self.generate_report = generate_report
        
        # åˆ†æç»“æœå­˜å‚¨
        self.enum_definitions: Dict[str, EnumDefinition] = {}
        self.enum_usages: List[EnumUsage] = []
        self.hardcoded_strings: List[EnumUsage] = []
        self.consistency_issues: List[ConsistencyIssue] = []
        
        # åˆ†ç±»å­˜å‚¨
        self.frontend_enums: Dict[str, Dict[str, str]] = {}
        self.backend_enums: Dict[str, Dict[str, str]] = {}
        
        # å·²çŸ¥çš„æšä¸¾å€¼æ¨¡å¼
        self.known_enum_patterns = {
            'parse_status': r'^(uploaded|parsing|completed|failed)$',
            'task_status': r'^(pending|running|completed|failed|cancelled)$',
            'user_status': r'^(active|inactive|pending|suspended)$',
            'log_level': r'^(debug|info|warning|error|fatal)$',
            'file_type': r'^(excel|csv|json|xml|pdf)$'
        }
        
    def analyze_project(self) -> Dict[str, Any]:
        """åˆ†ææ•´ä¸ªé¡¹ç›®çš„æšä¸¾ä½¿ç”¨æƒ…å†µ"""
        print("ğŸ” å¼€å§‹é¡¹ç›®æšä¸¾ä½¿ç”¨åœºæ™¯åˆ†æ...")
        print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        # 1. åˆ†æåç«¯Pythonä»£ç 
        print("ğŸ“‚ åˆ†æåç«¯Pythonæšä¸¾...")
        self._analyze_backend_enums()
        
        # 2. åˆ†æå‰ç«¯TypeScriptä»£ç   
        print("ğŸ“‚ åˆ†æå‰ç«¯TypeScriptæšä¸¾...")
        self._analyze_frontend_enums()
        
        # 3. æŸ¥æ‰¾ç¡¬ç¼–ç å­—ç¬¦ä¸²
        print("ğŸ” æŸ¥æ‰¾ç¡¬ç¼–ç æšä¸¾å­—ç¬¦ä¸²...")
        self._find_hardcoded_enum_strings()
        
        # 4. äº¤å‰éªŒè¯å‰åç«¯ä¸€è‡´æ€§
        print("ğŸ”„ æ£€æŸ¥å‰åç«¯æšä¸¾ä¸€è‡´æ€§...")
        self._check_frontend_backend_consistency()
        
        # 5. ç”Ÿæˆåˆ†ææŠ¥å‘Š
        return self._generate_analysis_report()
    
    def _analyze_backend_enums(self):
        """åˆ†æåç«¯Pythonæšä¸¾å®šä¹‰å’Œä½¿ç”¨"""
        backend_dir = Path("backend/app")
        if not backend_dir.exists():
            print("âš ï¸ åç«¯ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡åç«¯åˆ†æ")
            return
        
        python_files = list(backend_dir.rglob("*.py"))
        if self.verbose:
            print(f"   æ‰¾åˆ° {len(python_files)} ä¸ªPythonæ–‡ä»¶")
        
        for py_file in python_files:
            if self.verbose:
                print(f"   åˆ†ææ–‡ä»¶: {py_file}")
            self._analyze_python_file(py_file)
        
        print(f"âœ… åç«¯åˆ†æå®Œæˆ: å‘ç° {len(self.backend_enums)} ä¸ªæšä¸¾å®šä¹‰")
    
    def _analyze_python_file(self, file_path: Path):
        """åˆ†æå•ä¸ªPythonæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except (UnicodeDecodeError, FileNotFoundError):
            return
        
        try:
            tree = ast.parse(content)
        except SyntaxError:
            if self.verbose:
                print(f"     è¯­æ³•é”™è¯¯ï¼Œè·³è¿‡æ–‡ä»¶: {file_path}")
            return
        
        # ä½¿ç”¨ASTåˆ†ææšä¸¾å®šä¹‰
        enum_visitor = EnumDefinitionVisitor(file_path, 'python')
        enum_visitor.visit(tree)
        
        for enum_def in enum_visitor.enums:
            self.enum_definitions[f"{enum_def.name}_backend"] = enum_def
            self.backend_enums[enum_def.name] = enum_def.values
        
        # æŸ¥æ‰¾æšä¸¾ä½¿ç”¨
        usage_visitor = EnumUsageVisitor(file_path, self.enum_definitions, 'python')
        usage_visitor.visit(tree)
        self.enum_usages.extend(usage_visitor.usages)
        
        # æŸ¥æ‰¾å¯èƒ½çš„ç¡¬ç¼–ç å­—ç¬¦ä¸²
        self._find_hardcoded_in_python_file(file_path, content)
    
    def _analyze_frontend_enums(self):
        """åˆ†æå‰ç«¯TypeScriptæšä¸¾å®šä¹‰å’Œä½¿ç”¨"""
        frontend_dir = Path("frontend/src")
        if not frontend_dir.exists():
            print("âš ï¸ å‰ç«¯ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡å‰ç«¯åˆ†æ")
            return
        
        ts_files = list(frontend_dir.rglob("*.ts")) + list(frontend_dir.rglob("*.tsx"))
        if self.verbose:
            print(f"   æ‰¾åˆ° {len(ts_files)} ä¸ªTypeScriptæ–‡ä»¶")
        
        for ts_file in ts_files:
            if self.verbose:
                print(f"   åˆ†ææ–‡ä»¶: {ts_file}")
            self._analyze_typescript_file(ts_file)
        
        print(f"âœ… å‰ç«¯åˆ†æå®Œæˆ: å‘ç° {len(self.frontend_enums)} ä¸ªæšä¸¾å®šä¹‰")
    
    def _analyze_typescript_file(self, file_path: Path):
        """åˆ†æå•ä¸ªTypeScriptæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except (UnicodeDecodeError, FileNotFoundError):
            return
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†æTypeScriptæšä¸¾
        self._find_typescript_enums(file_path, content)
        self._find_hardcoded_in_typescript_file(file_path, content)
    
    def _find_typescript_enums(self, file_path: Path, content: str):
        """æŸ¥æ‰¾TypeScriptæšä¸¾å®šä¹‰"""
        # åŒ¹é…enumå®šä¹‰
        enum_pattern = r'enum\s+(\w+)\s*\{([^}]*)\}'
        enums = re.finditer(enum_pattern, content, re.DOTALL)
        
        for match in enums:
            enum_name = match.group(1)
            enum_body = match.group(2)
            line_num = content[:match.start()].count('\n') + 1
            
            # æå–æšä¸¾å€¼
            enum_values = {}
            value_pattern = r'(\w+)\s*=\s*[\'"]([^\'\"]*)[\'"]'
            values = re.finditer(value_pattern, enum_body)
            
            for value_match in values:
                key = value_match.group(1)
                value = value_match.group(2)
                enum_values[key] = value
            
            # å­˜å‚¨æšä¸¾å®šä¹‰
            enum_def = EnumDefinition(
                name=enum_name,
                file_path=str(file_path),
                line_number=line_num,
                values=enum_values,
                base_classes=[],
                language='typescript'
            )
            
            self.enum_definitions[f"{enum_name}_frontend"] = enum_def
            self.frontend_enums[enum_name] = enum_values
        
        # åŒ¹é…typeè”åˆç±»å‹ï¼ˆç±»ä¼¼æšä¸¾ï¼‰
        type_pattern = r'type\s+(\w+)\s*=\s*([^;]+);'
        types = re.finditer(type_pattern, content)
        
        for match in types:
            type_name = match.group(1)
            type_def = match.group(2).strip()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å­—ç¬¦ä¸²è”åˆç±»å‹
            if "'" in type_def or '"' in type_def:
                line_num = content[:match.start()].count('\n') + 1
                
                # æå–å­—ç¬¦ä¸²å€¼
                string_values = re.findall(r'[\'"]([^\'"]*)[\'"]', type_def)
                if string_values:
                    enum_values = {f"VALUE_{i}": value for i, value in enumerate(string_values)}
                    
                    enum_def = EnumDefinition(
                        name=type_name,
                        file_path=str(file_path),
                        line_number=line_num,
                        values=enum_values,
                        base_classes=[],
                        language='typescript'
                    )
                    
                    self.enum_definitions[f"{type_name}_frontend"] = enum_def
                    self.frontend_enums[type_name] = enum_values
    
    def _find_hardcoded_in_python_file(self, file_path: Path, content: str):
        """åœ¨Pythonæ–‡ä»¶ä¸­æŸ¥æ‰¾ç¡¬ç¼–ç æšä¸¾å­—ç¬¦ä¸²"""
        # æŸ¥æ‰¾å­—ç¬¦ä¸²å­—é¢é‡
        string_pattern = r'[\'"]([^\'"]+)[\'"]'
        matches = re.finditer(string_pattern, content)
        
        for match in matches:
            string_value = match.group(1)
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…å·²çŸ¥æšä¸¾æ¨¡å¼
            for enum_type, pattern in self.known_enum_patterns.items():
                if re.match(pattern, string_value.lower()):
                    line_num = content[:match.start()].count('\n') + 1
                    context = self._get_line_context(content, match.start())
                    
                    self.hardcoded_strings.append(EnumUsage(
                        enum_name=enum_type,
                        usage_type='hardcoded',
                        file_path=str(file_path),
                        line_number=line_num,
                        context=context,
                        value=string_value,
                        language='python'
                    ))
    
    def _find_hardcoded_in_typescript_file(self, file_path: Path, content: str):
        """åœ¨TypeScriptæ–‡ä»¶ä¸­æŸ¥æ‰¾ç¡¬ç¼–ç æšä¸¾å­—ç¬¦ä¸²"""
        # æŸ¥æ‰¾å­—ç¬¦ä¸²å­—é¢é‡
        string_pattern = r'[\'"]([^\'"]+)[\'"]'
        matches = re.finditer(string_pattern, content)
        
        for match in matches:
            string_value = match.group(1)
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…å·²çŸ¥æšä¸¾æ¨¡å¼
            for enum_type, pattern in self.known_enum_patterns.items():
                if re.match(pattern, string_value.lower()):
                    line_num = content[:match.start()].count('\n') + 1
                    context = self._get_line_context(content, match.start())
                    
                    self.hardcoded_strings.append(EnumUsage(
                        enum_name=enum_type,
                        usage_type='hardcoded',
                        file_path=str(file_path),
                        line_number=line_num,
                        context=context,
                        value=string_value,
                        language='typescript'
                    ))
    
    def _find_hardcoded_enum_strings(self):
        """åœ¨æ‰€æœ‰æ–‡ä»¶ä¸­æŸ¥æ‰¾ç¡¬ç¼–ç æšä¸¾å­—ç¬¦ä¸²"""
        total_hardcoded = len(self.hardcoded_strings)
        
        if total_hardcoded > 0:
            print(f"âš ï¸ å‘ç° {total_hardcoded} ä¸ªå¯èƒ½çš„ç¡¬ç¼–ç æšä¸¾å­—ç¬¦ä¸²")
            
            if self.verbose:
                by_type = defaultdict(int)
                for usage in self.hardcoded_strings:
                    by_type[usage.enum_name] += 1
                
                for enum_type, count in by_type.items():
                    print(f"   {enum_type}: {count} ä¸ª")
        else:
            print("âœ… æœªå‘ç°æ˜æ˜¾çš„ç¡¬ç¼–ç æšä¸¾å­—ç¬¦ä¸²")
    
    def _check_frontend_backend_consistency(self):
        """æ£€æŸ¥å‰åç«¯æšä¸¾ä¸€è‡´æ€§"""
        # å¯»æ‰¾å¯èƒ½å¯¹åº”çš„å‰åç«¯æšä¸¾
        potential_matches = []
        
        for frontend_enum, frontend_values in self.frontend_enums.items():
            for backend_enum, backend_values in self.backend_enums.items():
                # åŸºäºåç§°ç›¸ä¼¼æ€§åŒ¹é…
                similarity_score = self._calculate_name_similarity(frontend_enum, backend_enum)
                if similarity_score > 0.6:  # 60%ä»¥ä¸Šç›¸ä¼¼åº¦
                    potential_matches.append((frontend_enum, backend_enum, similarity_score))
        
        # å¯¹åŒ¹é…ç»“æœè¿›è¡Œåˆ†æ
        for frontend_enum, backend_enum, score in potential_matches:
            frontend_values = set(self.frontend_enums[frontend_enum].values())
            backend_values = set(self.backend_enums[backend_enum].values())
            
            # æ£€æŸ¥å€¼çš„ä¸€è‡´æ€§
            missing_in_frontend = backend_values - frontend_values
            missing_in_backend = frontend_values - backend_values
            
            if missing_in_frontend:
                self.consistency_issues.append(ConsistencyIssue(
                    issue_type='missing_enum_values',
                    description=f'å‰ç«¯æšä¸¾ {frontend_enum} ç¼ºå°‘å€¼: {missing_in_frontend}',
                    frontend_item=frontend_enum,
                    backend_item=backend_enum,
                    severity='warning'
                ))
            
            if missing_in_backend:
                self.consistency_issues.append(ConsistencyIssue(
                    issue_type='missing_enum_values', 
                    description=f'åç«¯æšä¸¾ {backend_enum} ç¼ºå°‘å€¼: {missing_in_backend}',
                    frontend_item=frontend_enum,
                    backend_item=backend_enum,
                    severity='warning'
                ))
        
        if self.consistency_issues:
            print(f"âš ï¸ å‘ç° {len(self.consistency_issues)} ä¸ªå‰åç«¯ä¸€è‡´æ€§é—®é¢˜")
        else:
            print("âœ… å‰åç«¯æšä¸¾ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡")
    
    def _calculate_name_similarity(self, name1: str, name2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªåç§°çš„ç›¸ä¼¼åº¦"""
        name1_lower = name1.lower()
        name2_lower = name2.lower()
        
        # ç®€å•çš„ç›¸ä¼¼åº¦è®¡ç®—
        if name1_lower == name2_lower:
            return 1.0
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…±åŒçš„å…³é”®è¯
        keywords1 = set(re.findall(r'\w+', name1_lower))
        keywords2 = set(re.findall(r'\w+', name2_lower))
        
        if not keywords1 or not keywords2:
            return 0.0
        
        intersection = keywords1 & keywords2
        union = keywords1 | keywords2
        
        return len(intersection) / len(union)
    
    def _get_line_context(self, content: str, position: int, context_lines: int = 1) -> str:
        """è·å–æŒ‡å®šä½ç½®çš„ä»£ç ä¸Šä¸‹æ–‡"""
        lines = content[:position].split('\n')
        line_num = len(lines) - 1
        all_lines = content.split('\n')
        
        start = max(0, line_num - context_lines)
        end = min(len(all_lines), line_num + context_lines + 1)
        
        context_lines_content = []
        for i in range(start, end):
            prefix = ">>> " if i == line_num else "    "
            context_lines_content.append(f"{prefix}{all_lines[i]}")
        
        return '\n'.join(context_lines_content)
    
    def _generate_analysis_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("\n" + "="*50)
        print("ğŸ“Š æšä¸¾ä½¿ç”¨åœºæ™¯åˆ†ææŠ¥å‘Š")
        print("="*50)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_enums = len(self.enum_definitions)
        frontend_enum_count = len(self.frontend_enums)
        backend_enum_count = len(self.backend_enums)
        hardcoded_count = len(self.hardcoded_strings)
        consistency_issues_count = len(self.consistency_issues)
        
        print(f"\nğŸ“ˆ ç»Ÿè®¡æ¦‚è§ˆ:")
        print(f"   æšä¸¾å®šä¹‰æ€»æ•°: {total_enums}")
        print(f"   å‰ç«¯æšä¸¾: {frontend_enum_count}")
        print(f"   åç«¯æšä¸¾: {backend_enum_count}")
        print(f"   ç¡¬ç¼–ç å­—ç¬¦ä¸²: {hardcoded_count}")
        print(f"   ä¸€è‡´æ€§é—®é¢˜: {consistency_issues_count}")
        
        # è¯¦ç»†çš„æšä¸¾åˆ—è¡¨
        if self.verbose:
            if self.frontend_enums:
                print(f"\nğŸ“‚ å‰ç«¯æšä¸¾è¯¦æƒ…:")
                for name, values in self.frontend_enums.items():
                    print(f"   â€¢ {name}: {list(values.values())}")
            
            if self.backend_enums:
                print(f"\nğŸ“‚ åç«¯æšä¸¾è¯¦æƒ…:")
                for name, values in self.backend_enums.items():
                    print(f"   â€¢ {name}: {list(values.values())}")
        
        # ç¡¬ç¼–ç å­—ç¬¦ä¸²é—®é¢˜
        if hardcoded_count > 0:
            print(f"\nâš ï¸ ç¡¬ç¼–ç æšä¸¾å­—ç¬¦ä¸²é—®é¢˜:")
            hardcoded_by_type = defaultdict(list)
            for usage in self.hardcoded_strings:
                hardcoded_by_type[usage.enum_name].append(usage)
            
            for enum_type, usages in hardcoded_by_type.items():
                print(f"   ğŸ“‹ {enum_type} ({len(usages)} å¤„):")
                for usage in usages[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    print(f"      â€¢ {usage.file_path}:{usage.line_number} - '{usage.value}'")
                if len(usages) > 3:
                    print(f"      â€¢ ... è¿˜æœ‰ {len(usages) - 3} å¤„")
        
        # ä¸€è‡´æ€§é—®é¢˜
        if consistency_issues_count > 0:
            print(f"\nğŸ”„ å‰åç«¯ä¸€è‡´æ€§é—®é¢˜:")
            for issue in self.consistency_issues:
                print(f"   â€¢ {issue.description}")
        
        # ç”ŸæˆæŠ¥å‘Šæ•°æ®
        report_data = {
            'summary': {
                'total_enums': total_enums,
                'frontend_enums': frontend_enum_count,
                'backend_enums': backend_enum_count,
                'hardcoded_strings': hardcoded_count,
                'consistency_issues': consistency_issues_count,
                'analysis_time': datetime.now().isoformat()
            },
            'enum_definitions': {k: asdict(v) for k, v in self.enum_definitions.items()},
            'hardcoded_strings': [asdict(usage) for usage in self.hardcoded_strings],
            'consistency_issues': [asdict(issue) for issue in self.consistency_issues]
        }
        
        # ä¿å­˜æŠ¥å‘Šæ–‡ä»¶
        if self.generate_report:
            report_file = Path('enum-usage-analysis-report.json')
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ“„ è¯¦ç»†åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        self._generate_improvement_suggestions()
        
        return report_data
    
    def _generate_improvement_suggestions(self):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        
        suggestions = []
        
        if len(self.hardcoded_strings) > 0:
            suggestions.append("1. å°†ç¡¬ç¼–ç å­—ç¬¦ä¸²æ›¿æ¢ä¸ºæšä¸¾å€¼ä½¿ç”¨")
            suggestions.append("   - åˆ›å»ºæˆ–ä½¿ç”¨ç°æœ‰æšä¸¾å®šä¹‰")
            suggestions.append("   - ç»Ÿä¸€æšä¸¾å€¼çš„å‘½åæ ¼å¼ï¼ˆå»ºè®®å°å†™ï¼‰")
        
        if len(self.consistency_issues) > 0:
            suggestions.append("2. è§£å†³å‰åç«¯æšä¸¾ä¸€è‡´æ€§é—®é¢˜")
            suggestions.append("   - ç»Ÿä¸€å‰åç«¯æšä¸¾å€¼å®šä¹‰")
            suggestions.append("   - å»ºç«‹æšä¸¾å€¼åŒæ­¥æœºåˆ¶")
        
        if len(self.frontend_enums) == 0 or len(self.backend_enums) == 0:
            suggestions.append("3. å®Œå–„æšä¸¾å®šä¹‰")
            suggestions.append("   - ä¸ºçŠ¶æ€å€¼åˆ›å»ºæ˜ç¡®çš„æšä¸¾å®šä¹‰")
            suggestions.append("   - é¿å…åœ¨ä»£ç ä¸­ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²å­—é¢é‡")
        
        if not suggestions:
            suggestions.append("âœ… å½“å‰æšä¸¾ä½¿ç”¨ç¬¦åˆæœ€ä½³å®è·µ")
        
        for suggestion in suggestions:
            print(f"   {suggestion}")

class EnumDefinitionVisitor(ast.NodeVisitor):
    """ASTè®¿é—®å™¨ï¼šæŸ¥æ‰¾Pythonæšä¸¾å®šä¹‰"""
    
    def __init__(self, file_path: Path, language: str):
        self.file_path = file_path
        self.language = language
        self.enums: List[EnumDefinition] = []
        
    def visit_ClassDef(self, node: ast.ClassDef):
        # æ£€æŸ¥æ˜¯å¦ç»§æ‰¿è‡ªEnum
        base_names = []
        for base in node.bases:
            if isinstance(base, ast.Name):
                base_names.append(base.id)
            elif isinstance(base, ast.Attribute):
                base_names.append(f"{base.value.id}.{base.attr}" if isinstance(base.value, ast.Name) else base.attr)
        
        if any('Enum' in name for name in base_names):
            enum_values = {}
            
            for item in node.body:
                if isinstance(item, ast.Assign):
                    # å¤„ç†èµ‹å€¼è¯­å¥ ENUM_KEY = "enum_value"
                    if len(item.targets) == 1 and isinstance(item.targets[0], ast.Name):
                        key = item.targets[0].id
                        if isinstance(item.value, ast.Constant):
                            enum_values[key] = str(item.value.value)
            
            self.enums.append(EnumDefinition(
                name=node.name,
                file_path=str(self.file_path),
                line_number=node.lineno,
                values=enum_values,
                base_classes=base_names,
                language=self.language
            ))
        
        self.generic_visit(node)

class EnumUsageVisitor(ast.NodeVisitor):
    """ASTè®¿é—®å™¨ï¼šæŸ¥æ‰¾Pythonæšä¸¾ä½¿ç”¨"""
    
    def __init__(self, file_path: Path, enum_definitions: Dict[str, EnumDefinition], language: str):
        self.file_path = file_path
        self.enum_definitions = enum_definitions
        self.language = language
        self.usages: List[EnumUsage] = []
    
    def visit_Attribute(self, node: ast.Attribute):
        # æŸ¥æ‰¾ EnumClass.VALUE å½¢å¼çš„ä½¿ç”¨
        if isinstance(node.value, ast.Name):
            enum_name = node.value.id
            # æ£€æŸ¥æ˜¯å¦æ˜¯å·²çŸ¥æšä¸¾
            for def_key, enum_def in self.enum_definitions.items():
                if enum_def.name == enum_name and enum_def.language == self.language:
                    self.usages.append(EnumUsage(
                        enum_name=enum_name,
                        usage_type='direct',
                        file_path=str(self.file_path),
                        line_number=node.lineno,
                        context=f"{enum_name}.{node.attr}",
                        value=node.attr,
                        language=self.language
                    ))
        
        self.generic_visit(node)

def main():
    parser = argparse.ArgumentParser(description='Excel2Docæšä¸¾ä½¿ç”¨åœºæ™¯åˆ†æå·¥å…·')
    parser.add_argument('--verbose', '-v', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡º')
    parser.add_argument('--report', '-r', action='store_true', help='ç”Ÿæˆè¯¦ç»†JSONåˆ†ææŠ¥å‘Š')
    
    args = parser.parse_args()
    
    if not Path('frontend').exists() and not Path('backend').exists():
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°frontendæˆ–backendç›®å½•")
        print("è¯·åœ¨Excel2Docé¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬")
        return 1
    
    analyzer = EnumUsageAnalyzer(verbose=args.verbose, generate_report=args.report)
    analyzer.analyze_project()
    
    print(f"\nğŸ‰ æšä¸¾ä½¿ç”¨åœºæ™¯åˆ†æå®Œæˆï¼")
    return 0

if __name__ == "__main__":
    exit(main())